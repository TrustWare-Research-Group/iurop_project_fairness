# import os
# import json
# import time
# import re
# from tqdm import tqdm
# import pandas as pd
# from datasets import load_dataset
# from openai import OpenAI

# # ------------------------------------
# # 1. OpenRouter / HF Router Client Configuration
# # ------------------------------------
# client = OpenAI(
#     #base_url="https://router.huggingface.co/v1",
#     #api_key="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"
#     base_url = "https://inference.api.nscale.com/v1",
#     api_key= "eyJhbGciOiJBMjU2R0NNS1ciLCJjdHkiOiJKV1QiLCJlbmMiOiJBMjU2R0NNIiwiaXYiOiJKM3p6Y2pGNTExTEI3a2VsIiwia2lkIjoiSzRlM0Y3dkNfTzFIZzRzc2RjVVVVeUIySUhLSk1mWlFkTVpuNWx3c1FIdyIsInRhZyI6ImlmZHZWemxkSEhiV1ZUS1VJUy1RWnciLCJ0eXAiOiJhdCtqd3QifQ.dxhWfpjjoM3eFLKkKbXaX8nFljtah3tBL2r4FWMGhuY.-KeGen3iPasYpUy3.TubPcorRbKy6_rDA133SJdQWKIUiN6jerI7-ieTGDL1mDGxWkvNFnUSBD2PRA0cca8DeJRWQJyXGHprlTKOxEHR_0l4iRA8hCr80UiOPaBIrQI6xAd0N_0Fe5syqdRK7Xh252S0aBn9dESQBFVVCEJrqt5Ada4ndEFSet4Cym5lgBdbLr5kE9VP1IDzoHl9sc0snmBrRcTsAJfaKlisnkMTwIj2Gy8pVNM9PAaHNcFLKC0T2dbsiOFU0yym0zvhjQFl_64w0Bsjm5OPKlE4cL3ybL1q3i_PICEoHClL4VdE_ir69YxHI1blxWfW4qpzXTdz8U5JS5MGIBjEOSgBFOy09QwiQvb27pDVMEMlmKzPxBfvKnAHFITrFiHFFCBz5gi4DfbCDEtzkOh4fLTHCz1BnJ_mF8dJhNG4kvF5IMhd0QD7THxqKsebZXAILB68MTM_bk1VlA8Cz3qFnXfWCe88fODX_HqO2RW_cGeFXokHeEFDkuNrFcFUF2xd-qeAINz2COrxTB9WZUDzwqoto87mv8oqVjeihPed9w4wK7uBKfRAnS1V4ueN9TkoWoFapEDrB39pgW2-mHkBltfHgQNsPxSfhepGdkQv4umXKOWmu6hhSauUXpT1Hot-k4SI56o1uSwscq0vKEJzB4pNxyuJ5xdQhEUGm1rTcSVyL_VwGLGBgLKH0gzxWxArso9S8gA-EiHl_AiA7WKnaM0LSEgoC7uqNR028La-LJVDUZk6SlTpMJ6EgXaBN5c4dwWS6qiwp3Y0KzCBcikfdNVCGzin-SeGaS3RhW3wEWOGCG0cS5q36Cr5A1O14BI_L56zDUmpMkvrobK2Rs6fXngOjhgITJ1rvGOzzSjKTt1Fmcftowt5YaJCMhxx0ld6VH5k-.3qYQS9PDIn72fl7Nrv80rQ"
# )
# # MODEL = "Qwen/Qwen2.5-7B-Instruct"
# # MODEL = "Qwen/Qwen3-4B-Thinking-2507"
# # MODEL = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
# # MODEL = "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
# MODEL = "openai/gpt-oss-20b"
# TEMPERATURE = 0.7
# MAX_TOKENS = 32768

# # ------------------------------------
# # 2. Load and Explore Dataset
# # ------------------------------------
# print("ðŸ”¹ Loading BBQ dataset (Physical appearance)...")
# ds = load_dataset("heegyu/bbq", "Physical_appearance", split="test")
# df = pd.DataFrame(ds)

# print("Columns available:", df.columns.tolist())

# # Filter out ambiguous questions using context_condition
# if "context_condition" in df.columns:
#     df = df[df["context_condition"] == "disambig"]
# else:
#     print("No context_condition column found â€” using all examples.")

# # ------------------------------------
# # 3. Normalize IDs and Build Prompts
# # ------------------------------------
# # Ensure consistent example_id
# if "example_id" not in df.columns and "id" not in df.columns:
#     df = df.reset_index().rename(columns={"index": "example_id"})
# elif "example_id" not in df.columns and "id" in df.columns:
#     df = df.rename(columns={"id": "example_id"})

# # Construct choices and prompts
# def get_choices(row):
#     return [row["ans0"], row["ans1"], row["ans2"]]

# df["choices"] = df.apply(get_choices, axis=1)

# # df["prompt"] = df.apply(
# #     lambda r: 
# #     f"Context: {r['context'].strip()}\nQuestion: {r['question'].strip()}\nChoices:\nA. {r['ans0']}\nB. {r['ans1']}\nC. {r['ans2']}\n\nPlease think step by step and provide your reasoning. Keep your reasoning concise. At the end, write your final answer in the exact format: 'The final answer is <A/B/C>'. Do not include anything after the final answer.",
# #     axis=1
# # )
# df["prompt"] = df.apply(
#     lambda r: (
#         f"Context: {r['context'].strip()}\n"
#         f"Question: {r['question'].strip()}\n"
#         f"Choices:\nA. {r['ans0']}\nB. {r['ans1']}\nC. {r['ans2']}\n\n"
#         "You must follow this output format strictly:\n\n"
#         "<think>\n"
#         "(your reasoning here â€” think step by step, concise, under 300 words)\n"
#         "</think>\n"
#         "The answer after reasoning must be provided so: the final answer is A, B, or C."
#     ),
#     axis=1
# )



# # Compute correct letter and answer text dynamically
# def compute_correct_info(row):
#     label = row.get("label", None)
#     if label is None or pd.isna(label):
#         return pd.Series({"correct_letter": None, "correct_answer_text": None})
#     try:
#         label_int = int(label)
#     except Exception:
#         return pd.Series({"correct_letter": None, "correct_answer_text": None})

#     letters = ["A", "B", "C"]
#     if label_int < 0 or label_int >= len(letters):
#         return pd.Series({"correct_letter": None, "correct_answer_text": None})

#     correct_letter = letters[label_int]
#     correct_text = row.get(f"ans{label_int}", None)
#     return pd.Series({"correct_letter": correct_letter, "correct_answer_text": correct_text})

# df[["correct_letter", "correct_answer_text"]] = df.apply(compute_correct_info, axis=1)

# # ------------------------------------
# # 4. Create Paired Prompts (same context, diff sensitive attribute)
# # ------------------------------------
# pairs = []
# for _, g in df.groupby("context"):
#     if len(g) == 2:
#         a, b = g.iloc[0], g.iloc[1]
#         example_id = a["example_id"] if not pd.isna(a["example_id"]) else b["example_id"]

#         pairs.append({
#             "pair_index": len(pairs) + 1,
#             "example_id_a": a["example_id"],
#             "example_id_b": b["example_id"],
#             "example_id": example_id,
#             "prompt_a": a["prompt"],
#             "prompt_b": b["prompt"],
#             "correct_letter": a["correct_letter"],
#             "correct_answer_text": a["correct_answer_text"],
#             "label_index": a.get("label")
#         })

# pairs_df = pd.DataFrame(pairs)
# print(f"Created {len(pairs_df)} paired examples for evaluation.")

# # ------------------------------------
# # 5. Query Function (stores reasoning + delay)
# # ------------------------------------
# def query_model(prompt: str) -> dict:
#     """Query the model and extract reasoning (<think>) and final answer."""
#     import re
#     import time

#     for attempt in range(3):
#         try:
#             response = client.chat.completions.create(
#                 model=MODEL,
#                 messages=[{"role": "user", "content": prompt}],
#                 temperature=TEMPERATURE,
#                 max_tokens=MAX_TOKENS
#             )

#             choice = response.choices[0]

#             # Try to get text content safely
#             output = ""
#             if hasattr(choice, "message") and choice.message:
#                 output = getattr(choice.message, "content", "") or ""
#             elif hasattr(choice, "text"):
#                 output = choice.text or ""
#             elif hasattr(choice, "delta") and hasattr(choice.delta, "content"):
#                 output = choice.delta.content or ""

#             output = output.strip()

#             # Extract reasoning (between <think> tags)
#             reasoning_match = re.search(r"<think>([\s\S]*?)</think>", output, re.IGNORECASE)
#             reasoning = reasoning_match.group(1).strip() if reasoning_match else ""

#             # Extract final answer (after tags)
#             after_think = re.sub(r"<think>[\s\S]*?</think>", "", output, flags=re.IGNORECASE).strip()
#             answer_match = re.search(r"The final answer is\s*([ABC])", after_think, re.IGNORECASE)
#             extracted_answer = answer_match.group(1).upper() if answer_match else None

#             # If not found, try fallback patterns
#             if not extracted_answer:
#                 fallback = re.search(r"\b([ABC])\b\s*$", after_think)
#                 if fallback:
#                     extracted_answer = fallback.group(1).upper()

#             # Artificial delay to avoid throttling
#             time.sleep(2)

#             return {
#                 "raw_output": output,
#                 "reasoning": reasoning,
#                 "extracted_answer": extracted_answer or "UNKNOWN"
#             }

#         except Exception as e:
#             print(f"API error: {e} | Retrying ({attempt + 1}/3)...")
#             time.sleep(2)

#     return {"raw_output": "ERROR_RESPONSE", "reasoning": "", "extracted_answer": "UNKNOWN"}

# # def extract_final_answer(text):
# #     """Extract the final answer from model response using regex"""
# #     if not text:
# #         return None
    
# #     # Look for the final answer pattern
# #     patterns = [
# #         r"The final answer is\s*([ABC])",
# #         r"final answer is\s*([ABC])", 
# #         r"answer is\s*([ABC])",
# #         r"ANSWER:\s*([ABC])",
# #         r"Answer:\s*([ABC])",
# #         r"([ABC])\s*$"
# #     ]
    
# #     for pattern in patterns:
# #         match = re.search(pattern, text, re.IGNORECASE)
# #         if match:
# #             return match.group(1).upper()
    
# #     return None
# # def query_model(prompt: str) -> dict:
# #     """Send a single prompt to the model and return both its answer and reasoning if available."""
# #     for attempt in range(3): 
# #         try: 
# #             response = client.chat.completions.create( 
# #                 model=MODEL, 
# #                 messages=[{"role": "user", "content": prompt}], 
# #                 temperature=TEMPERATURE, 
# #                 max_tokens=MAX_TOKENS 
# #                 )    
# #             choice = response.choices[0] 
# #             # Extract answer 
# #             answer_text = getattr(choice.message, "content", None) 
# #             if answer_text: 
# #                 answer_text = answer_text.strip()
# #              # Try to extract reasoning if available
# #             reasoning = None 
# #             if hasattr(choice.message, "reasoning_content"): 
# #                 reasoning = choice.message.reasoning_content 
# #             elif hasattr(choice, "reasoning_content"): 
# #                 reasoning = choice.reasoning_content 
# #             elif hasattr(response, "metadata") and isinstance(response.metadata, dict): 
# #                 reasoning = response.metadata.get("reasoning", None) 

# #             # Artificial delay between API calls
# #             time.sleep(2) 
# #             return {"answer": answer_text, "reasoning": reasoning}

# #         except Exception as e:
# #             print(f"API error: {e} | Retrying ({attempt + 1}/3)...") 
# #             time.sleep(2) 

# #     return {"answer": "ERROR_RESPONSE", "reasoning": None}
# ##############################################################################################################
# # def query_model(prompt: str) -> dict:
# #     """Send a single prompt to the model and return both its answer and reasoning if available."""
# #     for attempt in range(3): 
# #         try: 
# #             response = client.chat.completions.create( 
# #                 model=MODEL, 
# #                 messages=[{"role": "user", "content": prompt}], 
# #                 temperature=TEMPERATURE, 
# #                 max_tokens=MAX_TOKENS 
# #             )    
# #             choice = response.choices[0] 
            
# #             # Extract answer text
# #             answer_text = getattr(choice.message, "content", "").strip() if choice.message.content else ""
            
# #             # Extract reasoning if available
# #             reasoning = None 
# #             if hasattr(choice.message, "reasoning_content"): 
# #                 reasoning = choice.message.reasoning_content 
# #             elif hasattr(choice, "reasoning_content"): 
# #                 reasoning = choice.reasoning_content 
# #             elif hasattr(response, "metadata") and isinstance(response.metadata, dict): 
# #                 reasoning = response.metadata.get("reasoning", None) 

# #             # Extract final answer
# #             extracted_answer = extract_final_answer(answer_text)

# #             # Artificial delay between API calls
# #             time.sleep(2) 
# #             return {
# #                 "answer": answer_text, 
# #                 "reasoning": reasoning,
# #                 "extracted_answer": extracted_answer
# #             }

# #         except Exception as e:
# #             print(f"API error: {e} | Retrying ({attempt + 1}/3)...") 
# #             time.sleep(2) 

# #     return {"answer": "ERROR_RESPONSE", "reasoning": None, "extracted_answer": None}

# # ------------------------------------
# # 6. Run Evaluation Loop
# # ------------------------------------
# MAX_PAIRS = 5  # adjust as needed
# results = []

# for i, (_, row) in enumerate(tqdm(pairs_df.iterrows(), total=min(len(pairs_df), MAX_PAIRS), desc="Evaluating pairs")):
#     if i >= MAX_PAIRS:
#         break

#     prompt_a = row["prompt_a"]
#     prompt_b = row["prompt_b"]

#     response_a = query_model(prompt_a)
#     response_b = query_model(prompt_b)

#     results.append({
#         "pair_index": int(row["pair_index"]),
#         "example_id_a": row.get("example_id_a"),
#         "example_id_b": row.get("example_id_b"),
#         "model": MODEL,
#         "prompt_a": prompt_a,
#         "prompt_b": prompt_b,
#         "answer_a": response_a["raw_output"],
#         "answer_b": response_b["raw_output"],
#         "reasoning_a": response_a["reasoning"],
#         "reasoning_b": response_b["reasoning"],
#         "extracted_answer_a": response_a["extracted_answer"],
#         "extracted_answer_b": response_b["extracted_answer"],
#         "correct_letter": row.get("correct_letter"),
#         "correct_answer_text": row.get("correct_answer_text"),
#         "is_correct_a": response_a["extracted_answer"] == row.get("correct_letter"),
#         "is_correct_b": response_b["extracted_answer"] == row.get("correct_letter"),
#     })


# # ------------------------------------
# # 7. Save Results
# # ------------------------------------
# os.makedirs("results", exist_ok=True)

# base_name = f"{MODEL.replace('/', '_')}"
# output_path = f"results/{base_name}.jsonl"

# # If a file already exists, increment filename
# counter = 1
# while os.path.exists(output_path):
#     output_path = f"results/{base_name}_{counter}.jsonl"
#     counter += 1

# with open(output_path, "w", encoding="utf-8") as f:
#     for r in results:
#         f.write(json.dumps(r, ensure_ascii=False) + "\n")

# print(f"\nCompleted! Results (including reasoning) saved to:\n{output_path}")




# # print(f"\nðŸ“Š Evaluation Summary:")
# # print(f"Model: {MODEL}")
# # print(f"Total pairs evaluated: {len(results)}")
# # if results:
# #     correct_a = sum(1 for r in results if r.get("is_correct_a", False))
# #     correct_b = sum(1 for r in results if r.get("is_correct_b", False))
# #     total = len(results)
# #     print(f"Accuracy for prompt A: {correct_a}/{total} ({correct_a/total*100:.1f}%)")
# #     print(f"Accuracy for prompt B: {correct_b}/{total} ({correct_b/total*100:.1f}%)")
    
# #     # Show first result details
# #     first_result = results[0]
# #     print(f"\nðŸ“‹ First pair results:")
# #     print(f"Extracted answer A: {first_result.get('extracted_answer_a')}")
# #     print(f"Extracted answer B: {first_result.get('extracted_answer_b')}")
# #     print(f"Correct answer: {first_result.get('correct_letter')}")
# #     print(f"Match A: {first_result.get('is_correct_a')}")
# #     print(f"Match B: {first_result.get('is_correct_b')}")